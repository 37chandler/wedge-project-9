{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wedge Task 1\n",
    "___\n",
    "In this first task, we will work on uploading the Wedge data from around 50 zip files to Google BigQuery (GBQ). To make this process easier, we will initially run our script on a smaller subset of the Wedge data. This smaller subset is structured the same way as the larger dataset, which allows us to test the entire process effectively. Once we have processed the smaller subset and successfully uploaded it to GBQ, we will then change the directory settings in our script to point to the full dataset. This way, we can ensure everything works correctly before handling the entire volume of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unpack the Zip Files\n",
    "___\n",
    "The first thing we need to do is extract the csvs from the zip files located in our data directory. We will first work with our subset of Wedge zip files. After the full script has run successfully, we will return to this point to change which directory we are processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets a file path, attaching the file name to the data directory\n",
    "def get_file_path(data_directory, file):\n",
    "    file_path = os.path.join(data_directory, file)\n",
    "    return file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpack the zips and save the csvs to an output directory\n",
    "def unpack_and_save_csv(zip_file_path, output_directory):\n",
    "    # Open the zip file at the specified path\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as Z:\n",
    "        # Get the name of the single CSV file inside the zip\n",
    "        file = Z.namelist()[0]\n",
    "        # Create the output directory if it doesn't exist\n",
    "        if not os.path.exists(output_directory):\n",
    "            os.makedirs(output_directory)\n",
    "        # Extract the file to the output directory\n",
    "        Z.extract(file, path=output_directory)\n",
    "        extracted_file_path = get_file_path(output_directory, file)\n",
    "        # Read the extracted CSV into a pandas DataFrame\n",
    "        contents = pd.read_csv(extracted_file_path)\n",
    "        # Return the file name and the DataFrame with contents\n",
    "        return file, contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/71/xfbtbhlj7376x9dz94q5nmbh0000gn/T/ipykernel_51033/2829811161.py:14: DtypeWarning: Columns (33) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  contents = pd.read_csv(extracted_file_path)\n",
      "/var/folders/71/xfbtbhlj7376x9dz94q5nmbh0000gn/T/ipykernel_51033/2829811161.py:14: DtypeWarning: Columns (18,36,37,41,43,44,48) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  contents = pd.read_csv(extracted_file_path)\n",
      "/var/folders/71/xfbtbhlj7376x9dz94q5nmbh0000gn/T/ipykernel_51033/2829811161.py:14: DtypeWarning: Columns (18,36,37,41,43,44,48) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  contents = pd.read_csv(extracted_file_path)\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 28] No space left on device",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m zip_file_path \u001b[38;5;241m=\u001b[39m get_file_path(data_directory, zip_file)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Unpack the zips and save the csvs to an output directory\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[43munpack_and_save_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mzip_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcsv_directory\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 11\u001b[0m, in \u001b[0;36munpack_and_save_csv\u001b[0;34m(zip_file_path, output_directory)\u001b[0m\n\u001b[1;32m      9\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(output_directory)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Extract the file to the output directory\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[43mZ\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_directory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m extracted_file_path \u001b[38;5;241m=\u001b[39m get_file_path(output_directory, file)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Read the extracted CSV into a pandas DataFrame\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/zipfile.py:1664\u001b[0m, in \u001b[0;36mZipFile.extract\u001b[0;34m(self, member, path, pwd)\u001b[0m\n\u001b[1;32m   1661\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1662\u001b[0m     path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mfspath(path)\n\u001b[0;32m-> 1664\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extract_member\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmember\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpwd\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/zipfile.py:1736\u001b[0m, in \u001b[0;36mZipFile._extract_member\u001b[0;34m(self, member, targetpath, pwd)\u001b[0m\n\u001b[1;32m   1732\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m targetpath\n\u001b[1;32m   1734\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopen(member, pwd\u001b[38;5;241m=\u001b[39mpwd) \u001b[38;5;28;01mas\u001b[39;00m source, \\\n\u001b[1;32m   1735\u001b[0m      \u001b[38;5;28mopen\u001b[39m(targetpath, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m target:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[43mshutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopyfileobj\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m targetpath\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/shutil.py:200\u001b[0m, in \u001b[0;36mcopyfileobj\u001b[0;34m(fsrc, fdst, length)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m buf:\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m fdst_write(buf)\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device"
     ]
    }
   ],
   "source": [
    "data_directory = \"data/WedgeZipOfZips/\"\n",
    "csv_directory = \"data/WedgeCSVs/\"\n",
    "\n",
    "# Loop through all zip files in the data directory\n",
    "for zip_file in os.listdir(data_directory):\n",
    "    if zip_file.endswith('.zip'):\n",
    "        # Get the full path of the zip file\n",
    "        zip_file_path = get_file_path(data_directory, zip_file)\n",
    "        # Unpack the zips and save the csvs to an output directory\n",
    "        unpack_and_save_csv(zip_file_path, csv_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Header Validation\n",
    "___\n",
    "Now, let's check to make sure each of the files in our csv directory are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets the headers (column names) from a directory of csvs\n",
    "# Will output a dataframe of the counts of different headers\n",
    "def get_headers(csv_directory):\n",
    "    headers = []\n",
    "    # Loop over the csvs\n",
    "    for csv_file in os.listdir(csv_directory):\n",
    "        # Get the csv file path\n",
    "        csv_file_path = get_file_path(csv_directory, csv_file)\n",
    "        # Read in the first row of the csv, which is the header\n",
    "        df = pd.read_csv(csv_file_path, nrows=0) \n",
    "        # Make the header a tuple and append it to our list\n",
    "        headers.append(tuple(df.columns)) \n",
    "    # Create the dataframe of header counts\n",
    "    headers_df = pd.DataFrame(Counter(headers).items(), columns=['Header', 'Count']).sort_values(by='Count', ascending=False)\n",
    "    # Return the dataframe of header counts\n",
    "    return headers_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the headers\n",
    "headers_df = get_headers(csv_directory)\n",
    "headers_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(headers_df.iloc[0,0])\n",
    "print(headers_df.iloc[1,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like on of the problems we have is differently delimited files. Some have columns separated by commas, others by semicolons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for csv_file in os.listdir(csv_directory):\n",
    "    # Get the csv file path\n",
    "    csv_file_path = get_file_path(csv_directory, csv_file)\n",
    "    # Determine the current delimiter by checking the first line of the file\n",
    "    with open(csv_file_path, 'r') as f:\n",
    "        first_line = f.readline().strip()\n",
    "        current_delimiter = ',' if ',' in first_line else ';'  \n",
    "    # Read the CSV file with the current delimiter\n",
    "    df = pd.read_csv(csv_file_path, delimiter=current_delimiter)    \n",
    "    # Save the file back with a semicolon (;) delimiter\n",
    "    df.to_csv(csv_file_path, sep=';', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the headers\n",
    "headers_df = get_headers(csv_directory)\n",
    "headers_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it looks like we've addresses the different delimiters, but we still have multiple data sets that don't have a header. To alleviate this, we will functionally add headers to csvs that do not have headers. We do this by comparing the header to the most common header. If the header is not the same as the most common header, we create a new data set with that header, append the data, and replace the file that didn't initially have the header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find teh most common header\n",
    "most_common_header = headers_df.iloc[0,0]\n",
    "# Loop over our csv files\n",
    "for csv_file in os.listdir(csv_directory):\n",
    "    # Get the csv file path\n",
    "    csv_file_path = get_file_path(csv_directory, csv_file)\n",
    "    # Read the csv file to a pandas df\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    # Check if the first row matches the most common header\n",
    "    if tuple(df.columns) != most_common_header:\n",
    "        df = pd.read_csv(csv_file_path, header=None)\n",
    "        df.columns = list(most_common_header)\n",
    "        # Overwrite the original csv file with the new header\n",
    "        df.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the headers\n",
    "headers_df = get_headers(csv_directory)\n",
    "headers_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome, now all of the csv files have the same delimiter and have common headers.\n",
    "## Double Quotation Handling\n",
    "___\n",
    "Now, when first investigating the files, there are some instances where lines are not read in correctly. This was mainly due to there being multiple double quotations as a value in an oject column. To avoid complication, we remove the quotations from the entire data set. When doing this, however, we end up with some formatting issues where double quotes are used to differentiate between lines. We remove trailing semicolons, which happens to fix all of our issues with the double quotes issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes double quotes and fixes several data sets with formatting issues\n",
    "def remove_double_quotes(csv_directory):\n",
    "    # Loop over our file of csvs\n",
    "    for csv_file in os.listdir(csv_directory):\n",
    "        # Get the file path of the csv\n",
    "        csv_file_path = os.path.join(csv_directory, csv_file)\n",
    "        # Read the csv file as a raw text file to handle quotes more flexibly\n",
    "        with open(csv_file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.readlines()\n",
    "        # Remove all double quotes and trim trailing semicolons\n",
    "        cleaned_content = []\n",
    "        for line in content:\n",
    "            # Remove double quotes and trailing white space\n",
    "            line = line.replace('\"', '').rstrip()\n",
    "            # Remove last semicolon\n",
    "            if line.endswith(';'):\n",
    "                line = line[:-1] \n",
    "            # Add the cleaned content to our list.\n",
    "            cleaned_content.append(line + '\\n')\n",
    "        # Write the cleaned content back to the CSV file\n",
    "        with open(csv_file_path, 'w', encoding='utf-8') as file:\n",
    "            file.writelines(cleaned_content) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove double quotes\n",
    "remove_double_quotes(csv_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigating Data Types\n",
    "___\n",
    "Now that we have all our data commonly delimited, with headers, and no other pertinent issues on a file by file basis, we will shift the focus to combining the csvs. To do this, we want to ensure that each dataset has the same data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets the data types from our directory of csvs\n",
    "# Will output a dataframe of the counts of different datatypes\n",
    "def get_datatypes(csv_directory):\n",
    "    datatypes = []\n",
    "    # Loop over the csvs\n",
    "    for csv_file in os.listdir(csv_directory):\n",
    "        # Get the file path of the csvs\n",
    "        csv_file_path = get_file_path(csv_directory, csv_file)\n",
    "        # Read the file as a pandas df\n",
    "        df = pd.read_csv(csv_file_path, delimiter=\";\", low_memory=False)  \n",
    "        # Find the data types and add them to our list\n",
    "        datatypes.append(tuple(df.dtypes))\n",
    "    # Create the dataframe of data type counts\n",
    "    datatypes_df = pd.DataFrame(Counter(datatypes).items(), columns=['Header', 'Count']).sort_values(by='Count', ascending=False)\n",
    "    # Return the dataframe of data type counts\n",
    "    return datatypes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the data types\n",
    "datatypes_df = get_datatypes(csv_directory)\n",
    "datatypes_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that not every data set has the same data type. So, we need to identify where we are having differences and make the appropriate changes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the datatypes into a dataframe\n",
    "def get_datatypes_df(csv_directory):\n",
    "    datatypes_list = [] \n",
    "    # Loop over the csvs\n",
    "    for csv_file in os.listdir(csv_directory):\n",
    "        # Get the file path of the csvs\n",
    "        csv_file_path = get_file_path(csv_directory, csv_file)\n",
    "        # Read the data into a pandas df\n",
    "        df = pd.DataFrame(pd.read_csv(csv_file_path, delimiter=\";\", low_memory=False))  \n",
    "        # Get the data types\n",
    "        datatypes_list.append(df.dtypes)\n",
    "    # Create the data frame of data types\n",
    "    datatypes_df = pd.DataFrame(datatypes_list)\n",
    "    # Return the data frame of data types\n",
    "    return datatypes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the data types\n",
    "datatypes_df = get_datatypes_df(csv_directory)\n",
    "datatypes_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than roll through each data set, we will assume that the most common data type is what should be expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets the columns and most common data type in a dictionary\n",
    "def check_common_datatype(csv_directory):\n",
    "    # Return the data types df\n",
    "    datatypes_df = get_datatypes_df(csv_directory)\n",
    "    common_dtype_dict = {}\n",
    "    # Get the most common data type and assign it to the dictionary\n",
    "    for column in datatypes_df.columns:\n",
    "        common_dtype = datatypes_df[column].mode()[0]\n",
    "        common_dtype_dict[column] = common_dtype\n",
    "    # Return the data type dictionary\n",
    "    return common_dtype_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the dictionary\n",
    "dtype_mapping = check_common_datatype(csv_directory)\n",
    "dtype_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds the data types so that they are mapped to what is in the dictionary\n",
    "def fix_datatypes(csv_directory, mapping=None):\n",
    "    # Keep track of how many columns need their data type changed\n",
    "    needing_changed = 0\n",
    "    # Loop over the csvs\n",
    "    for csv_file in os.listdir(csv_directory):\n",
    "        # Get the file path of the csvx]s\n",
    "        csv_file_path = get_file_path(csv_directory, csv_file)\n",
    "        try:\n",
    "            # Read in the data\n",
    "            df = pd.read_csv(csv_file_path, delimiter=\";\", dtype=mapping)  \n",
    "            print(csv_file)\n",
    "            print('-------')\n",
    "            # Print if the column doesn't have the right data type\n",
    "            for column in df.columns:\n",
    "                if df[column].dtype != dtype_mapping[column]:\n",
    "                    print(f\"{column}: {df[column].dtype} should be converted to {dtype_mapping[column]}.\")\n",
    "                    needing_changed += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {csv_file}: {e}\")\n",
    "    # Return the number of columns needing data type changed\n",
    "    return f\"{needing_changed} columns need their data types changed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_datatypes(csv_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actually change the data types of columns in the csvs\n",
    "def change_datatypes(csv_directory):\n",
    "    # Loop over the csvs\n",
    "    for csv_file in os.listdir(csv_directory):\n",
    "        # Get the file path of the csv\n",
    "        csv_file_path = get_file_path(csv_directory, csv_file)\n",
    "        # Read in the data\n",
    "        df = pd.read_csv(csv_file_path, delimiter=\";\")  \n",
    "        # Empty the display column. We have no use for this column.\n",
    "        df['display'] = ''\n",
    "        print(csv_file)\n",
    "        print('-------')\n",
    "        # Loop over the columns in the csv    \n",
    "        for column in df.columns:\n",
    "            # Check if we have the right data type\n",
    "            if df[column].dtype == dtype_mapping[column]:\n",
    "                continue\n",
    "            # Perform the conversion based on target type\n",
    "            try:\n",
    "                # Object conversion\n",
    "                if dtype_mapping[column] == 'object':\n",
    "                    # Replace nulls with blanks\n",
    "                    df[column] = df[column].replace({\n",
    "                        '\\\\N': '',\n",
    "                        '\\\\N.1': '',\n",
    "                        '\\\\N.2': '',\n",
    "                        '\\\\N.3': ''\n",
    "                        })\n",
    "                    # Object conversion\n",
    "                    df[column] = df[column].astype('object')\n",
    "                # Float conversion\n",
    "                elif dtype_mapping[column] == 'float64':\n",
    "                    # Ensure the column is treated as string first\n",
    "                    if df[column].dtype != 'object':\n",
    "                        df[column] = df[column].astype('str')\n",
    "                    # Modifies any places where we have strings that look like 2.4900.1\n",
    "                    df[column] = df[column].str.replace(r'\\.(\\d{2}|\\d{4})\\.\\d', r'.\\1', regex=True)\n",
    "                    # Replace nulls with 0s\n",
    "                    df[column] = df[column].replace({\n",
    "                        '\\\\N': 0,\n",
    "                        '\\\\N.1': 0,\n",
    "                        '\\\\N.2': 0,\n",
    "                        '\\\\N.3': 0,\n",
    "                        '\\\\N.4': 0,\n",
    "                        '\\\\N.5': 0,\n",
    "                        '\\\\N.6': 0,\n",
    "                        ' ': 0})\n",
    "                    # Replacing \"Unnamed\" with 0.0 for the float conversion\n",
    "                    if df[column].dtype == 'object':\n",
    "                        df[column] = df[column].replace(r'.*Unnamed.*', '0.0', regex=True)\n",
    "                    # Float conversion                  \n",
    "                    df[column] = df[column].astype('float64')\n",
    "                # Integer conversion\n",
    "                elif dtype_mapping[column] == 'int64':\n",
    "                    # Ensure the column is treated as string first\n",
    "                    if df[column].dtype != 'object':\n",
    "                        df[column] = df[column].astype('str')\n",
    "                    # Replace the nulls with 0s\n",
    "                    df[column] = df[column].replace({\n",
    "                        '\\\\N': 0,\n",
    "                        '\\\\N.1': 0,\n",
    "                        '\\\\N.2': 0,\n",
    "                        '\\\\N.3': 0,\n",
    "                        '\\\\N.4': 0,\n",
    "                        '\\\\N.5': 0,\n",
    "                        '\\\\N.6': 0,\n",
    "                        ' ': 0\n",
    "                        })\n",
    "                    # Convert to float first, then to integer\n",
    "                    df[column] = pd.to_numeric(df[column], errors='coerce').fillna(0).astype('float64')\n",
    "                    # Integer conversion\n",
    "                    df[column] = df[column].astype('int64')\n",
    "                else:\n",
    "                    print(f\"{column}: Unsupported target type {dtype_mapping[column]}.\")\n",
    "                # Trying to handle the display column\n",
    "                if df['display'].dtype == 'float64':\n",
    "                    df[column] = df[column].astype('object')\n",
    "            except Exception as e:\n",
    "                print(f\"Error converting {column} from {df[column].dtype} to {dtype_mapping[column]}: {e}\")\n",
    "        # Save the file back with a semicolon (;) delimiter\n",
    "        df.to_csv(csv_file_path, sep=';', index=False)\n",
    "    return \"Changes complete.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_datatypes(csv_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_datatypes(csv_directory, mapping=dtype_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datatypes_df = get_datatypes_df(csv_directory)\n",
    "\n",
    "# Initialize flag to track if any column has differing values\n",
    "all_same = True\n",
    "\n",
    "# Check if all values in each column are the same\n",
    "for column in datatypes_df.columns:\n",
    "    if datatypes_df[column].nunique() != 1:\n",
    "        print(f\"Values in column '{column}' are not the same.\")\n",
    "        all_same = False\n",
    "\n",
    "# If no column with differing values was found, print all values are the same\n",
    "if all_same:\n",
    "    print(\"All values in all columns are the same.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenating the Data Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to hold data frames\n",
    "df_list = []\n",
    "\n",
    "# Loop through all the files in the folder\n",
    "for filename in os.listdir(csv_directory):\n",
    "    if filename.endswith('.csv'):  # Only process csv files\n",
    "        # Get the file path of the csv\n",
    "        csv_file_path = get_file_path(csv_directory, csv_file)\n",
    "        # Read in the data set as a pandas data frame\n",
    "        df = pd.read_csv(csv_file_path, delimiter=\";\", low_memory=False, dtype=dtype_mapping) \n",
    "        # Append the dataframe to the list\n",
    "        df_list.append(df)\n",
    "\n",
    "# Concatenate all DataFrames into one\n",
    "big_df = pd.concat(df_list, ignore_index=True)\n",
    "big_df['display'] = big_df['display'].fillna('')\n",
    "file_name = 'data/transactions.csv'\n",
    "big_df.to_csv(file_name, sep=';', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check and notify about datatype mismatches\n",
    "for column, expected_dtype in dtype_mapping.items():\n",
    "    if column in big_df.columns and str(big_df[column].dtype) != expected_dtype:\n",
    "        print(f\"Column '{column}': Actual type = {big_df[column].dtype}, Expected type = {expected_dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To the Cloud\n",
    "___\n",
    "Finally, we'd like to take our data set and upload it to Google Big Query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a BigQuery client\n",
    "client = bigquery.Client()\n",
    "\n",
    "# Define the dataset and table\n",
    "dataset_id = 'wedge-to-the-cloud.wedge_to_the_dataset'\n",
    "table_id = 'wedge-to-the-cloud.wedge_to_the_dataset.transactions'\n",
    "\n",
    "# Load data into BigQuery\n",
    "job_config = bigquery.LoadJobConfig(write_disposition=\"WRITE_TRUNCATE\")\n",
    "\n",
    "job = client.load_table_from_dataframe(\n",
    "    big_df, table_id, job_config=job_config)  # Make an API request.\n",
    "\n",
    "job.result()  # Wait for the job to complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After validating the data set has been uploaded to GBQ, we know we can run this script for both the subset and full set of zip files. We have set up a few places for error checking, but since we have built some error throwing around our functions, we have built something that can be used to reprocess the csvs if we need to reprocess them or add more data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
